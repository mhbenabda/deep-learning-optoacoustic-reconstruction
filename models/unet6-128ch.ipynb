{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131d7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.ao.quantization as tq\n",
    "from torchinfo import summary\n",
    "#print(torch.__version__)\n",
    "\n",
    "# The device is automatically set to GPU if available, otherwise CPU\n",
    "# If you want to force the device to CPU, you can change the line to\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# It is important that your model and all data are on the same device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5cbed",
   "metadata": {},
   "source": [
    "### 1- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6828f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(**kwargs):\n",
    "    \"\"\"\n",
    "    Get the training and test data. The data files are assumed to be in the\n",
    "    same directory as this script.\n",
    "\n",
    "    Args:\n",
    "    - kwargs: Additional arguments that you might find useful - not necessary\n",
    "\n",
    "    Returns:\n",
    "    - train_data_input: Tensor[N_train_samples, C, H, W]\n",
    "    - train_data_label: Tensor[N_train_samples, C, H, W]\n",
    "    - test_data_input: Tensor[N_test_samples, C, H, W]\n",
    "    where N_train_samples is the number of training samples, N_test_samples is\n",
    "    the number of test samples, C is the number of channels (1 for grayscale),\n",
    "    H is the height of the image, and W is the width of the image.\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    train_data_input = np.load(\"dataset5/all_train/input_128/train_input_all.npy\")\n",
    "    print('train input shape: ', train_data_input.shape)\n",
    "    train_data_label = np.load(\"dataset5/all_train/output/train_label_all.npy\")\n",
    "    # train_data_input = np.load(\"dataset5/training/input/train_input_3.npy\")\n",
    "    # print('train input shape: ', train_data_input.shape)\n",
    "    # train_data_label = np.load(\"dataset5/training/output/train_label_3.npy\")\n",
    "    \n",
    "    # Make the training data a tensor\n",
    "    train_data_input = torch.tensor(train_data_input, dtype=torch.float32)\n",
    "    train_data_label = torch.tensor(train_data_label, dtype=torch.float32)\n",
    "\n",
    "    # Load the test data\n",
    "    test_data_input = np.load(\"dataset5/all_test/input_128/test_input_all.npy\")\n",
    "    print('test input shape: ', test_data_input.shape)\n",
    "    test_data_label = np.load(\"dataset5/all_test/output/test_label_all.npy\")\n",
    "    # test_data_input = np.load(\"dataset5/testing/input/test_input_3.npy\")\n",
    "    # print('test input shape: ', test_data_input.shape)\n",
    "    # test_data_label = np.load(\"dataset5/testing/output/test_label_3.npy\")\n",
    "    \n",
    "    # Make the test data a tensor\n",
    "    test_data_input = torch.tensor(test_data_input, dtype=torch.float32)\n",
    "    test_data_label = torch.tensor(test_data_label, dtype=torch.float32)\n",
    "\n",
    "    # train_data = train_data / 255  # Normalize to [0,1]\n",
    "    # test_data_input = test_data_input / 255  \n",
    "\n",
    "    \n",
    "    #print(test_data_input[1,:,:,:])\n",
    "\n",
    "    # Visualize the training data if needed\n",
    "    # Set to False if you don't want to save the images\n",
    "    if True:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not Path(\"dataset5/train_image_output\").exists():\n",
    "            Path(\"dataset5/train_image_output\").mkdir()\n",
    "        for i in tqdm(range(10), desc=\"Plotting train images\"):\n",
    "            # Show the training and the target image side by side\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(train_data_input[i].squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Training Input\")\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Training Label\")\n",
    "            plt.imshow(train_data_label[i].squeeze(), cmap=\"gray\")\n",
    "\n",
    "            plt.savefig(f\"dataset5/train_image_output/image_{i}.png\")\n",
    "            plt.close()\n",
    "\n",
    "    return train_data_input, train_data_label, test_data_input, test_data_label\n",
    "\n",
    "# train_data_input, train_data_label, test_data_input, test_data_label = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63ea9f",
   "metadata": {},
   "source": [
    "### 2- Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c209d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\" [Conv2d => ReLU] x2 \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=k_size, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=k_size, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# (1, 128, 1024) -> (1, 64, 64)   \n",
    "class Unet6(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__() # Initialize the parent class\n",
    "        self.target_height = 128\n",
    "        self.target_width = 128\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = DoubleConv(in_channels, 32)\n",
    "        self.enc2 = DoubleConv(32, 64)\n",
    "        self.enc3 = DoubleConv(64, 128)\n",
    "        # self.enc4 = DoubleConv(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(1,4), stride=(1,4), padding=0),\n",
    "            nn.Conv2d(256, 256, kernel_size=(1,2), stride=(1,2), padding=0),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Skip connections\n",
    "        self.skip2  = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=(1,4), stride=(1,4), padding=0),\n",
    "            nn.Conv2d(32, 32, kernel_size=(1,2), stride=(1,2), padding=0),\n",
    "        )\n",
    "        self.skip3  = nn.Sequential(    # because stride 8 not supported by DPU\n",
    "            nn.Conv2d(64, 64, kernel_size=(1,4), stride=(1,4), padding=0),\n",
    "            nn.Conv2d(64, 64, kernel_size=(1,2), stride=(1,2), padding=0)\n",
    "        )\n",
    "        self.skip4  = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=(1,4), stride=(1,4), padding=0),\n",
    "            nn.Conv2d(128, 128, kernel_size=(1,2), stride=(1,2), padding=0),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up3 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "            # If I have batchnorm & ReLU at the end then the output can only be positive, while \n",
    "            # my image is positive/negative !!!!!\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e1p = self.pool(e1)\n",
    "        e2 = self.enc2(e1p) \n",
    "        e2p = self.pool(e2) \n",
    "        e3 = self.enc3(e2p)\n",
    "        e3p = self.pool(e3)\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e3p) # 256, 16, 64 -> 256, 16, 16\n",
    "        # Skip connections with convolution for resizing\n",
    "        s4 = self.skip4(e3)  # 256, 32, 128 -> 256, 32, 32 \n",
    "        s3 = self.skip3(e2)\n",
    "        s2 = self.skip2(e1)\n",
    "        # Decoder\n",
    "        d4 = self.up4(b) # 256, 16, 16 -> 256, 32, 32\n",
    "        d4c = torch.cat([s4, d4], dim=1)  # Concatenate along the channel dimension\n",
    "        d4d = self.dec4(d4c)  # 512, 32, 32 -> 128, 32, 32\n",
    "\n",
    "        d3 = self.up3(d4d)\n",
    "        d3c = torch.cat([s3, d3], dim=1)\n",
    "        d3d = self.dec3(d3c)  \n",
    "        \n",
    "        d2 = self.up2(d3d)\n",
    "        d2c = torch.cat([s2, d2], dim=1)  # Concatenate along the channel dimension\n",
    "        d2d = self.dec2(d2c)  # 128, 128, 128 -> 32, 128, 128\n",
    "\n",
    "        \n",
    "        return d2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b6238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# dummy = Unet6()\n",
    "# x = torch.randn(1, 1, 128, 1024)  # Example input (batch, channel, H, W)\n",
    "# y = dummy(x)\n",
    "# print(y.shape)  # Output segmentation mask shape (should be close to input dimensions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b89e5",
   "metadata": {},
   "source": [
    "### 3- Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1687a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data_input, train_data_label, **kwargs):\n",
    "    \"\"\"\n",
    "    Train the model. Fill in the details of the data loader, the loss function,\n",
    "    the optimizer, and the training loop.\n",
    "\n",
    "    Args:\n",
    "    - train_data_input: Tensor[N_train_samples, C, H, W]\n",
    "    - train_data_label: Tensor[N_train_samples, C, H, W]\n",
    "    - kwargs: Additional arguments that you might find useful - not necessary\n",
    "\n",
    "    Returns:\n",
    "    - model: torch.nn.Module\n",
    "    \"\"\"\n",
    "    model = Unet6()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss() # nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=10**-3, weight_decay=1e-5)\n",
    "    batch_size = 64\n",
    "    dataset = TensorDataset(train_data_input, train_data_label)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # shuffle data since neighbour frames are very similar\n",
    "\n",
    "    # Training loop\n",
    "    n_epochs = 30\n",
    "    best_loss = float('inf')\n",
    "    train_loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in tqdm(\n",
    "            data_loader, desc=f\"Training Epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad() # Clears old gradients from the previous step. PyTorch accumulates gradient by default\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            train_loss_history.append(loss)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                torch.save(model.state_dict(), \"best_unet6.pth\")\n",
    "\n",
    "            loss.backward() # Computes the gradients of the loss with respect to the model parameters.\n",
    "            optimizer.step() # Update model parameters using computed gradient\n",
    "        torch.save(model.state_dict(), \"unet6.pth\")\n",
    "        print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab961811",
   "metadata": {},
   "source": [
    "### 4- Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0392906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data_input, test_data_label):\n",
    "    \"\"\"\n",
    "    Uses your model to predict the ouputs for the test data.\n",
    "\n",
    "    Args:\n",
    "    - model: torch.nn.Module\n",
    "    - test_data_input: Tensor\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # model evaluated using MSE error\n",
    "    mse_criterion = nn.MSELoss() # nn.L1Loss()\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_data_input = test_data_input.to(device)\n",
    "        test_data_label = test_data_label.to(device)\n",
    "        # Predict the output batch-wise to avoid memory issues\n",
    "        test_data_output = []\n",
    "        # Can increase or decrease this batch size depending on your\n",
    "        # memory requirements of your computer / model\n",
    "        # This will not affect the performance of the model\n",
    "        batch_size = 64\n",
    "        for i in tqdm(\n",
    "            range(0, test_data_input.shape[0], batch_size),\n",
    "            desc=\"Predicting test output\",\n",
    "        ):\n",
    "            output = model(test_data_input[i : i + batch_size])\n",
    "            output = output #* 255\n",
    "            # Calculate MSE for the batch\n",
    "            label = test_data_label[i : i + batch_size]\n",
    "            mse = mse_criterion(output, label)\n",
    "            total_mse += mse.item() * output.shape[0]\n",
    "            count += output.shape[0]\n",
    "\n",
    "            test_data_output.append(output.cpu())\n",
    "        test_data_output = torch.cat(test_data_output)\n",
    "\n",
    "    # Calculate the average MSE\n",
    "    mean_mse = total_mse / count\n",
    "    print(f\"Mean Test MSE: {mean_mse:.4f}\")\n",
    "    # print(f\"Mean Test L1 Loss: {mean_mse:.4f}\")\n",
    "\n",
    "    # Save the output\n",
    "    test_data_output = test_data_output.numpy()\n",
    "    # Ensure all values are in the range [0, 255]\n",
    "    # save_data_clipped = np.clip(test_data_output, 0, 255)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        \"submit_this_test_data_output.npz\", data=test_data_output)\n",
    "\n",
    "    # Set to False if you don't want to save the images\n",
    "    if True:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not Path(\"dataset5/test_image_output\").exists():\n",
    "            Path(\"dataset5/test_image_output\").mkdir()\n",
    "        for i in tqdm(range(20), desc=\"Plotting test images\"):\n",
    "            # Show the training and the target image side by side\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title(\"Test Input\")\n",
    "            plt.imshow(test_data_input[i].squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(test_data_output[i].squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Test Output\")\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(test_data_label[i].squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "            plt.title(\"Test Label\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"dataset5/test_image_output/image_{i}.png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273857e",
   "metadata": {},
   "source": [
    "### 5- Main: run training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input shape:  (5040, 1, 128, 1024)\n",
      "test input shape:  (556, 1, 128, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting train images: 100%|██████████| 10/10 [00:00<00:00, 11.42it/s]\n",
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.8318983912467957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.8181939125061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.7765371203422546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.7833572030067444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.7468092441558838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.7680854797363281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 0.7247817516326904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 0.7404963374137878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 0.7342360019683838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 0.7711604237556458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.7005895972251892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 loss: 0.7312182784080505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 loss: 0.7041957974433899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 loss: 0.7177205681800842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 loss: 0.6540700793266296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 loss: 0.6691591143608093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 loss: 0.6666934490203857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 loss: 0.6746852993965149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 loss: 0.6384734511375427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 loss: 0.6598237752914429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 loss: 0.6478717923164368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 loss: 0.6250422596931458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 loss: 0.6211138367652893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23:  85%|████████▍ | 67/79 [12:58<02:23, 11.95s/it]"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "# Reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Load the data\n",
    "train_data_input, train_data_label, test_data_input, test_data_label = get_data()\n",
    "# Train the model\n",
    "model = train_model(train_data_input, train_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739770ea",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf60849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Unet6                                    [1, 1, 128, 128]          --\n",
       "├─DoubleConv: 1-1                        [1, 32, 128, 1024]        --\n",
       "│    └─Sequential: 2-1                   [1, 32, 128, 1024]        --\n",
       "│    │    └─Conv2d: 3-1                  [1, 32, 128, 1024]        320\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 32, 128, 1024]        64\n",
       "│    │    └─ReLU: 3-3                    [1, 32, 128, 1024]        --\n",
       "│    │    └─Conv2d: 3-4                  [1, 32, 128, 1024]        9,248\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 32, 128, 1024]        64\n",
       "│    │    └─ReLU: 3-6                    [1, 32, 128, 1024]        --\n",
       "├─MaxPool2d: 1-2                         [1, 32, 64, 512]          --\n",
       "├─DoubleConv: 1-3                        [1, 64, 64, 512]          --\n",
       "│    └─Sequential: 2-2                   [1, 64, 64, 512]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 64, 512]          18,496\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 64, 512]          128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 64, 512]          --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 64, 512]          36,928\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 64, 512]          128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 64, 512]          --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 32, 256]          --\n",
       "├─DoubleConv: 1-5                        [1, 128, 32, 256]         --\n",
       "│    └─Sequential: 2-3                   [1, 128, 32, 256]         --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 32, 256]         73,856\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 32, 256]         256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 32, 256]         --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 32, 256]         147,584\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 32, 256]         256\n",
       "│    │    └─ReLU: 3-18                   [1, 128, 32, 256]         --\n",
       "├─MaxPool2d: 1-6                         [1, 128, 16, 128]         --\n",
       "├─Sequential: 1-7                        [1, 128, 16, 16]          --\n",
       "│    └─Conv2d: 2-4                       [1, 256, 16, 128]         295,168\n",
       "│    └─BatchNorm2d: 2-5                  [1, 256, 16, 128]         512\n",
       "│    └─ReLU: 2-6                         [1, 256, 16, 128]         --\n",
       "│    └─Conv2d: 2-7                       [1, 256, 16, 32]          262,400\n",
       "│    └─Conv2d: 2-8                       [1, 256, 16, 16]          131,328\n",
       "│    └─Conv2d: 2-9                       [1, 128, 16, 16]          295,040\n",
       "│    └─BatchNorm2d: 2-10                 [1, 128, 16, 16]          256\n",
       "│    └─ReLU: 2-11                        [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-8                        [1, 128, 32, 32]          --\n",
       "│    └─Conv2d: 2-12                      [1, 128, 32, 64]          65,664\n",
       "│    └─Conv2d: 2-13                      [1, 128, 32, 32]          32,896\n",
       "├─Sequential: 1-9                        [1, 64, 64, 64]           --\n",
       "│    └─Conv2d: 2-14                      [1, 64, 64, 128]          16,448\n",
       "│    └─Conv2d: 2-15                      [1, 64, 64, 64]           8,256\n",
       "├─Sequential: 1-10                       [1, 32, 128, 128]         --\n",
       "│    └─Conv2d: 2-16                      [1, 32, 128, 256]         4,128\n",
       "│    └─Conv2d: 2-17                      [1, 32, 128, 128]         2,080\n",
       "├─ConvTranspose2d: 1-11                  [1, 128, 32, 32]          65,664\n",
       "├─Sequential: 1-12                       [1, 64, 32, 32]           --\n",
       "│    └─Conv2d: 2-18                      [1, 128, 32, 32]          295,040\n",
       "│    └─BatchNorm2d: 2-19                 [1, 128, 32, 32]          256\n",
       "│    └─ReLU: 2-20                        [1, 128, 32, 32]          --\n",
       "│    └─Conv2d: 2-21                      [1, 64, 32, 32]           73,792\n",
       "│    └─BatchNorm2d: 2-22                 [1, 64, 32, 32]           128\n",
       "│    └─ReLU: 2-23                        [1, 64, 32, 32]           --\n",
       "├─ConvTranspose2d: 1-13                  [1, 64, 64, 64]           16,448\n",
       "├─Sequential: 1-14                       [1, 32, 64, 64]           --\n",
       "│    └─Conv2d: 2-24                      [1, 64, 64, 64]           73,792\n",
       "│    └─BatchNorm2d: 2-25                 [1, 64, 64, 64]           128\n",
       "│    └─ReLU: 2-26                        [1, 64, 64, 64]           --\n",
       "│    └─Conv2d: 2-27                      [1, 32, 64, 64]           18,464\n",
       "│    └─BatchNorm2d: 2-28                 [1, 32, 64, 64]           64\n",
       "│    └─ReLU: 2-29                        [1, 32, 64, 64]           --\n",
       "├─ConvTranspose2d: 1-15                  [1, 32, 128, 128]         4,128\n",
       "├─Sequential: 1-16                       [1, 1, 128, 128]          --\n",
       "│    └─Conv2d: 2-30                      [1, 32, 128, 128]         18,464\n",
       "│    └─BatchNorm2d: 2-31                 [1, 32, 128, 128]         64\n",
       "│    └─ReLU: 2-32                        [1, 32, 128, 128]         --\n",
       "│    └─Conv2d: 2-33                      [1, 1, 128, 128]          289\n",
       "==========================================================================================\n",
       "Total params: 1,968,225\n",
       "Trainable params: 1,968,225\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 7.50\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 292.68\n",
       "Params size (MB): 7.87\n",
       "Estimated Total Size (MB): 301.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = Unet6()\n",
    "best_model.load_state_dict(torch.load('best_unet6-MSE-23ep.pth', weights_only=True))\n",
    "summary(best_model, input_size=(1, 1, 128, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6654c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test output: 100%|██████████| 9/9 [00:31<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test MSE: 0.7034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting test images: 100%|██████████| 20/20 [00:02<00:00,  6.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_data_input, train_data_label, test_data_input, test_data_label = get_data()\n",
    "test_model(best_model, test_data_input, test_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05f35573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on disk: 7.55 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of the model on disk\n",
    "size_mb = os.path.getsize(\"best_unet6-MSE-23ep.pth\") / 1024**2\n",
    "print(f\"Model size on disk: {size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13168f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test output: 100%|██████████| 9/9 [00:31<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test MSE: 0.8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting test images: 100%|██████████| 20/20 [00:02<00:00,  6.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test model trained with L1 loss\n",
    "\n",
    "best_model = Unet6()\n",
    "best_model.load_state_dict(torch.load('best_unet6-L1-30ep.pth', weights_only=True))\n",
    "test_model(best_model, test_data_input, test_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81182ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.086816 seconds\n",
      "Frames Per Second: 11.5 FPS\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Load your model (make sure it's in eval mode)\n",
    "model = Unet6()\n",
    "model.load_state_dict(torch.load('best_unet6-MSE-23ep.pth', weights_only=True))\n",
    "\n",
    "\n",
    "# Prepare your test input tensor (match your model's expected input)\n",
    "input_tensor = torch.randn(1, 1, 128, 1024)  \n",
    "\n",
    "# Warm-up (optional, ensures any lazy initializations are done)\n",
    "with torch.no_grad():\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "\n",
    "# Print inference time in seconds\n",
    "inf_time = end - start\n",
    "print(\"Inference time: {:.6f} seconds\".format(end - start))\n",
    "print(\"Frames Per Second: {:.1f} FPS\".format(1/inf_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
